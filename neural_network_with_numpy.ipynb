{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VyjXbngtbXKW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.W = {}\n",
        "    self.b = {}\n",
        "    self.dW = {}\n",
        "    self.db = {}\n",
        "    self.layer_infos = []\n",
        "  \n",
        "  def add_layer(self, n_neurons, activation, n_inputs = 0):\n",
        "    if n_inputs == 0:\n",
        "      last_key = list(self.W)[-1]\n",
        "      layer_weights =  np.random.randn(n_neurons,self.W[last_key].shape[0])\n",
        "      layer_biases = np.zeros((1, n_neurons))\n",
        "      self.W[\"W\" + str(int(last_key[-1]) + 1)] = layer_weights\n",
        "      self.b[\"b\" + str(int(last_key[-1]) + 1)] = layer_biases\n",
        "    else:\n",
        "      layer_weights = 0.10 * np.random.randn(n_neurons,n_inputs)\n",
        "      layer_biases = np.zeros((1, n_neurons))\n",
        "      self.W[\"W\" + str(1)] = layer_weights\n",
        "      self.b[\"b\" + str(1)] = layer_biases\n",
        "\n",
        "    self.layer_infos.append(activation)\n",
        "\n",
        "  def fit(self, X, y, epochs, learning_rate):\n",
        "    cost_history = []\n",
        "    accuracy_history = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "      y_pred, memory = self._forward(X)\n",
        "      cost = self._cost(y_pred, y)\n",
        "      cost_history.append(cost)\n",
        "      accuracy = self._accuracy(y_pred, y)\n",
        "      accuracy_history.append(accuracy)\n",
        "  \n",
        "\n",
        "      self._backward(y_pred, y, memory)\n",
        "\n",
        "      self._update(learning_rate)\n",
        "\n",
        "      print(\"accuracy : \" + str(accuracy) + \", lost : \" + str(cost) )\n",
        "\n",
        "    return cost_history, accuracy_history\n",
        "\n",
        "  def _forward(self, X):\n",
        "    memory = {}\n",
        "    A_current = X\n",
        "\n",
        "    for i in range(len(self.layer_infos)):\n",
        "      layer_index = i + 1\n",
        "\n",
        "      A_previous = A_current\n",
        "\n",
        "      if self.layer_infos[i] == \"sigmoid\":\n",
        "        X_current = np.dot(A_previous, self.W[\"W\" + str(layer_index)].T) + self.b[\"b\" + str(layer_index)]\n",
        "        A_current = self._sigmoid(X_current)\n",
        "        memory[\"X\" + str(layer_index)] = X_current\n",
        "      elif self.layer_infos[i] == \"relu\":\n",
        "        X_current = np.dot(A_previous, self.W[\"W\" + str(layer_index)].T) +  self.b[\"b\" + str(layer_index)]\n",
        "        A_current = self._relu(X_current)\n",
        "        memory[\"X\" + str(layer_index)] = X_current\n",
        "      \n",
        "      memory[\"A\" + str(i)] = A_previous\n",
        "      \n",
        "    return A_current, memory\n",
        "  \n",
        "  def _backward(self, y_pred, y, memory):\n",
        "    y = y.reshape(y_pred.shape)\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    dA_previous = - (np.divide(y, y_pred + epsilon) - np.divide( 1 - y, 1 - y_pred + epsilon))\n",
        "\n",
        "    for i in reversed(range(len(self.layer_infos))):\n",
        "      m = memory[\"A\" + str(i)].shape[1]\n",
        "      layer_index_current = i + 1\n",
        "\n",
        "\n",
        "      dA_current = dA_previous\n",
        "\n",
        "      if self.layer_infos[i] == \"sigmoid\":\n",
        "        dX_current = self._sigmoid_derivative(dA_current, memory[\"X\" + str(layer_index_current)])\n",
        "      if self.layer_infos[i] == \"relu\":\n",
        "        dX_current = self._relu_derivative(dA_current, memory[\"X\" + str(layer_index_current)])\n",
        "\n",
        "      dW_current = np.dot(dX_current.T, memory[\"A\" + str(i)]) / m\n",
        "      db_current = np.sum(dX_current, axis = 0, keepdims = True) / m\n",
        "      dA_previous = np.dot(dX_current,self.W[\"W\" + str(layer_index_current)])\n",
        "\n",
        "    \n",
        "      self.dW[\"dW\" + str(layer_index_current)] = dW_current\n",
        "      self.db[\"db\" + str(layer_index_current)] = db_current\n",
        "\n",
        "  def _update(self,learning_rate):\n",
        "    for i in range(len(self.layer_infos)):\n",
        "      self.W[\"W\" + str(i+1)] += learning_rate * self.dW[\"dW\" + str(i+1)]\n",
        "      self.b[\"b\" + str(i+1)] += learning_rate * self.db[\"db\" + str(i+1)]\n",
        "\n",
        "  def _cost(self, y_pred, y):\n",
        "    m = y_pred.shape[1]\n",
        "    epsilon = 1e-5\n",
        "    cost = (-1/m) * (np.dot(y, np.log(y_pred + epsilon).T) + np.dot(1-y, np.log(1 - y_pred+epsilon).T))\n",
        "    return np.sum(cost)\n",
        "\n",
        "  def _accuracy(self, y_pred, y):\n",
        "    m = y_pred.shape[0]\n",
        "    y_pred[y_pred > 0.5] = 1\n",
        "    y_pred[y_pred <= 0.5] = 0\n",
        "    accuracy = np.sum(y == y_pred) / m\n",
        "    return accuracy\n",
        "\n",
        "  def _sigmoid(self, X):\n",
        "    return 1 / ( 1 + np.exp(-X))\n",
        "\n",
        "  def _relu(self, X):\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "  def _sigmoid_derivative(self, dA, X):\n",
        "    sig = self._sigmoid(X)\n",
        "    return dA * sig * ( 1 - sig )\n",
        "\n",
        "  def _relu_derivative(self, dA, X):\n",
        "    dX = np.array(dA, copy = True)\n",
        "    dX[X <= 0] = 0\n",
        "    return dX\n",
        "\n"
      ],
      "metadata": {
        "id": "1rxmVRyvbcUw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork()\n",
        "\n",
        "model.add_layer(20, activation = \"relu\", n_inputs = 4096)\n",
        "model.add_layer(20, activation = \"relu\")\n",
        "\n",
        "model.add_layer(1, activation = \"sigmoid\")"
      ],
      "metadata": {
        "id": "eXbsq4Qmchgd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/content/drive/MyDrive/Colab Notebooks/data.npy\")\n",
        "\n",
        "np.random.shuffle(data)\n",
        "\n",
        "slicer = int(data.shape[0]*0.9)\n",
        "X_train = data[:slicer,:-1]\n",
        "X_test = data[slicer:data.shape[0],:-1]\n",
        "y_train = data[:slicer,-1]\n",
        "y_train = y_train.reshape(len(y_train),1)\n",
        "y_test = data[slicer:data.shape[0],-1]\n",
        "y_test = y_test.reshape(len(y_test),1)"
      ],
      "metadata": {
        "id": "51hpeOrswk6z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost, accuracy = model.fit(X_train,y_train, 3 , 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjsVUD2HcwCX",
        "outputId": "e296dba5-bc78-46b7-f95d-31263349f18d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.48739495798319327, lost : 5326243.297012291\n",
            "accuracy : 0.48739495798319327, lost : 5326243.297012291\n",
            "accuracy : 0.48739495798319327, lost : 5326243.297012291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IrIJtQ4ZtI-S"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}